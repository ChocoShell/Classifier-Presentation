{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random Forest is an ensemble approach that can also be through of as a form of nearest neighbor predictor.[^1]\n",
    "\n",
    "Ensembles are groups of learners working together to create a strong learner.  The gray ones are a good guess at the data but the red curve is much better.[^2]\n",
    "\n",
    "[^1]: http://blog.citizennet.com/blog/2012/11/10/random-forests-ensembles-and-performance-metrics\n",
    "[^2]: https://blog.citizennet.com/hs-fs/hubfs/Imported_Blog_Media/skitch.png?t=1529595390188&width=482&name=skitch.png\n",
    "\n",
    "Random forest is kind of universal. It can predict any kind, (categorical or continous) any columns, pixels, zipcodes, revenues.  In general it doesn't overfit and if it does, it is easy to fix.  Don't need a separate validation set.  It has few if any statistical assumptions. It doesn't assume normal distributions, or linear, or that you specify the interactions.  Very few feature engineering. Doesn't need much math manipulation.\n",
    "\n",
    "Curse of dimensionality - The more columns you have, you create a space that is more and more empty.\n",
    "\n",
    "What are the pros cons? Why it works good?\n",
    "\n",
    "Why use RMSLE -> sum((log(actual) - log(predicted))^2)\n",
    "I believe RMSLE is usually used when you don't want to penalize huge differences in the predicted and true values when both predicted and true values are huge numbers.\n",
    "https://stats.stackexchange.com/a/110610\n",
    "\n",
    "We take ten bad models and can make one good model because their errors are not correlated with eachother due to them creating trees off of different chunks of data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tennis data untouched\n",
      "   Day   Outlook Humidity    Wind Play\n",
      "0    1     Sunny     High    Weak   No\n",
      "1    2     Sunny     High  Strong   No\n",
      "2    3  Overcast     High    Weak  Yes\n",
      "3    4      Rain     High    Weak  Yes\n",
      "4    5      Rain   Normal    Weak  Yes\n",
      "\n",
      "Tennis Input Data with Categories\n",
      "     Outlook  Humidity  Wind\n",
      "Day                         \n",
      "1          2         0     1\n",
      "2          2         0     0\n",
      "3          0         0     1\n",
      "4          1         0     1\n",
      "5          1         1     1\n",
      "\n",
      "Tennis Output Data with Categories\n",
      "Day\n",
      "1    0\n",
      "2    0\n",
      "3    1\n",
      "4    1\n",
      "5    1\n",
      "Name: Play, dtype: int8\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "tennis = pd.read_csv(\"tennis.csv\")\n",
    "\n",
    "print(\"Tennis data untouched\")\n",
    "print(tennis.head())\n",
    "\n",
    "# Using Day as index\n",
    "tennis.set_index('Day', inplace=True)\n",
    "\n",
    "# Changing all categorical columns to numbers\n",
    "for column in tennis.columns.values:\n",
    "    tennis[column] = tennis[column].astype('category')\n",
    "\n",
    "tennis = tennis.apply(lambda x: x.cat.codes)\n",
    "\n",
    "# Splitting data into input data and output data\n",
    "tennis_x = tennis.drop('Play', axis=1)\n",
    "tennis_y = tennis['Play']\n",
    "\n",
    "\n",
    "print(\"\\nTennis Input Data with Categories\")\n",
    "\"\"\"\n",
    "Outlook:\n",
    "    Sunny = 2\n",
    "    Overcast = 0\n",
    "    Rain = 1\n",
    "\n",
    "Humidity:\n",
    "    High = 0\n",
    "    Normal = 1\n",
    "\n",
    "Wind:\n",
    "    Weak = 1\n",
    "    Strong = 0\n",
    "\n",
    "Play:\n",
    "    No = 0\n",
    "    Yes = 1\n",
    "\"\"\"\n",
    "print(tennis_x.head())\n",
    "print(\"\\nTennis Output Data with Categories\")\n",
    "print(tennis_y.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "dtclf = DecisionTreeClassifier()\n",
    "dtmodel = dtclf.fit(tennis_x, tennis_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\python27\\lib\\site-packages\\sklearn\\tree\\export.py:399: DeprecationWarning: out_file can be set to None starting from 0.18. This will be the default in 0.20.\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import graphviz\n",
    "import os\n",
    "from sklearn.tree import export_graphviz\n",
    "\n",
    "dot_data = export_graphviz(\n",
    "    dtmodel,\n",
    "    feature_names=tennis_x.columns,\n",
    "    filled=True,\n",
    "    rounded=True\n",
    ") \n",
    "graph = graphviz.Source(dot_data) \n",
    "\n",
    "os.system('dot -Tpng tree.dot -o tree.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest\n",
    "\n",
    "**Random Forest** is a statistical algorithm that is used to cluster points of data in groups. When the data set is large and/or there are many variables it becomes difficult to cluster the data because not all variables can be taken into account, therefore the algorithm can also give a certain chance that a data point belongs in a certain group."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This 'forest' is made up of multiple decision trees.\n",
    "\n",
    "This is a very popular machine learning algorithm and the go to algorithm for a lot of users."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Trees\n",
    "\n",
    "![titanic](titanic_tree.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Advantages of Decision Tress\n",
    " - Simple to understand, interpret and visualize.\n",
    " - Implicitly perform feature selection\n",
    " - Handles numerical or categorical data\n",
    " - Not much effort needed in setting up data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Disadvantages\n",
    " - May create overally complex trees that cause overfitting instead of generalizing the data.\n",
    " - They can become unstable because of small variations in data which can be fixed with methods known as bagging and boosting.\n",
    " - Because decision trees are **greedy** they go for the best answer for the next step, which may cause us to get stuck in a local minimum and not the global minimum that we want."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Decision Tree Example\n",
    "We want to predict whether he will play tennis based on this data\n",
    "![tennis](tennis.PNG)\n",
    "\n",
    "Here is our initial tree structure.  When the sky is overcast, he has a 100% chance to play tennis.  This is called a pure subset.  Since the other choices lead to impure subsets, we should split them even further.\n",
    "![playing](playingBaseball.PNG)\n",
    "\n",
    "Once we split all the choices down into pure subsets, we can accurately predict what days John will play tennis.\n",
    "![playing](baseballSplit.PNG)\n",
    "\n",
    "This leads us to the simplified tree below.\n",
    "![final](finalTree.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Back to Random Forests\n",
    "\n",
    "Random Forests are just multiple decision trees used in conjuction.  In a classifcation problem, the multiple decision trees vote on the outcome, while in a regression problem, you will take the average accross all trees.\n",
    "\n",
    "Its basic principle is that one will be worse at predictions as opposed to a group similar to contestants asking the audience to vote on 'Who wants to be a Millionaire?'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Advantages\n",
    "- Handles missing values and maintains accuracy when a large proportion fo the data is missing\n",
    "- It won't overfit for multiple trees.\n",
    "- Can handle large data sets with more dimensions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Disadvantages\n",
    "- Good job at classifcation but not great for regression.\n",
    "- Very 'black box'. You do not know what's going on inside them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Applications\n",
    "- Finding loyal customers at a store/bank, etc.\n",
    "- Helpful for diagnosing diseases based on past medical history.\n",
    "- Stock Market: Can be used to figure out expected loss/profit for a particular stock\n",
    "- Recommendation engine: Finding likelyhood of customer liking a specific product.\n",
    "- Image Classifcation: Kinect uses it to track the body."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How are Random Forests Made?\n",
    "- Assume a total number of cases, N, in a training set.  Then we take a sample of these at random with replacement.\n",
    "- If there are M input variables/features, a number m<M is specified such that at each node, m variables are selected at random out of the M.  The best split on these m is used to split the node. The value of m is held constant while we grow the forest.\n",
    "- Each tree is grown as far as possible (overfitting if we were just using 1 decision tree) with no pruning (chopping off nodes)\n",
    "- Predict new data by voting for classifcation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random Forest is an ensemble machine learning algorithm meaning that the algorithm takes a weak algorithm and combines them into a strong one.\n",
    "\n",
    "They use a divide and conquer approach so a miniforest inside the random forest may detect some other prediction.  For example, if we are trying to figure out if an image is a hand or not, there may be mini forests that can detect fingers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest Terms\n",
    "\n",
    "**Bagging** (stands for **B**ootstrap **Agg**regat**ing**) is a way to decrease the variance of your prediction by generating additional data for training from your original dataset.  Because you are generating data, you are not improving the predicting power of the model, but decreasing the variance.\n",
    "\n",
    "**Boosting** is a two-step approach, where one first uses subsets of the original data to produce a series of averagely performing models and then \"boosts\" their performance by combining them together using a particular cost function (=majority vote). Unlike bagging, in the classical boosting the subset creation is not random and depends upon the performance of the previous models: every new subsets contains the elements that were (likely to be) misclassified by previous models.  Used to reduce bias(underfitting -> Won't be good at predicting) and variance(overfitting -> Too good at predicting the training data but isn't generalizable)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
