{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest\n",
    "\n",
    "**Random Forest** is a statistical algorithm that is used to cluster points of data in groups. When the data set is large and/or there are many variables it becomes difficult to cluster the data because not all variables can be taken into account, therefore the algorithm can also give a certain chance that a data point belongs in a certain group."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This 'forest' is made up of multiple decision trees.\n",
    "\n",
    "This is a very popular machine learning algorithm and the go to algorithm for a lot of users."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Trees\n",
    "\n",
    "![titanic](titanic_tree.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Advantages of Decision Tress\n",
    " - Simple to understand, interpret and visualize.\n",
    " - Implicitly perform feature selection\n",
    " - Handles numerical or categorical data\n",
    " - Not much effort needed in setting up data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Disadvantages\n",
    " - May create overally complex trees that cause overfitting instead of generalizing the data.\n",
    " - They can become unstable because of small variations in data which can be fixed with methods known as bagging and boosting.\n",
    " - Because decision trees are **greedy** they go for the best answer for the next step, which may cause us to get stuck in a local minimum and not the global minimum that we want."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Decision Tree Example\n",
    "We want to predict whether he will play tennis based on this data\n",
    "![tennis](tennis.PNG)\n",
    "\n",
    "Here is our initial tree structure.  When the sky is overcast, he has a 100% chance to play tennis.  This is called a pure subset.  Since the other choices lead to impure subsets, we should split them even further.\n",
    "![playing](playingBaseball.PNG)\n",
    "\n",
    "Once we split all the choices down into pure subsets, we can accurately predict what days John will play tennis.\n",
    "![playing](baseballSplit.PNG)\n",
    "\n",
    "This leads us to the simplified tree below.\n",
    "![final](finalTree.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Back to Random Forests\n",
    "\n",
    "Random Forests are just multiple decision trees used in conjuction.  In a classifcation problem, the multiple decision trees vote on the outcome, while in a regression problem, you will take the average accross all trees.\n",
    "\n",
    "Its basic principle is that one will be worse at predictions as opposed to a group similar to contestants asking the audience to vote on 'Who wants to be a Millionaire?'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Advantages\n",
    "- Handles missing values and maintains accuracy when a large proportion fo the data is missing\n",
    "- It won't overfit for multiple trees.\n",
    "- Can handle large data sets with more dimensions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Disadvantages\n",
    "- Good job at classifcation but not great for regression.\n",
    "- Very 'black box'. You do not know what's going on inside them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Applications\n",
    "- Finding loyal customers at a store/bank, etc.\n",
    "- Helpful for diagnosing diseases based on past medical history.\n",
    "- Stock Market: Can be used to figure out expected loss/profit for a particular stock\n",
    "- Recommendation engine: Finding likelyhood of customer liking a specific product.\n",
    "- Image Classifcation: Kinect uses it to track the body."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How are Random Forests Made?\n",
    "- Assume a total number of cases, N, in a training set.  Then we take a sample of these at random with replacement.\n",
    "- If there are M input variables/features, a number m<M is specified such that at each node, m variables are selected at random out of the M.  The best split on these m is used to split the node. The value of m is held constant while we grow the forest.\n",
    "- Each tree is grown as far as possible (overfitting if we were just using 1 decision tree) with no pruning (chopping off nodes)\n",
    "- Predict new data by voting for classifcation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random Forest is an ensemble machine learning algorithm meaning that the algorithm takes a weak algorithm and combines them into a strong one.\n",
    "\n",
    "They use a divide and conquer approach so a miniforest inside the random forest may detect some other prediction.  For example, if we are trying to figure out if an image is a hand or not, there may be mini forests that can detect fingers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest Terms\n",
    "\n",
    "**Bagging** (stands for **B**ootstrap **Agg**regat**ing**) is a way to decrease the variance of your prediction by generating additional data for training from your original dataset.  Because you are generating data, you are not improving the predicting power of the model, but decreasing the variance.\n",
    "\n",
    "**Boosting** is a two-step approach, where one first uses subsets of the original data to produce a series of averagely performing models and then \"boosts\" their performance by combining them together using a particular cost function (=majority vote). Unlike bagging, in the classical boosting the subset creation is not random and depends upon the performance of the previous models: every new subsets contains the elements that were (likely to be) misclassified by previous models.  Used to reduce bias(underfitting -> Won't be good at predicting) and variance(overfitting -> Too good at predicting the training data but isn't generalizable)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
